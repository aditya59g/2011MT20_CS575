# -*- coding: utf-8 -*-
"""Apple Stock Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17DufaezycAbe1c1ivwsvLIO5Mm30E8k2
"""

import pandas as pd
import numpy as np
from pandas_datareader import data as pdr
from datetime import datetime
import matplotlib.pyplot as plt 
from statsmodels.tsa.stattools import adfuller,kpss
from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
import warnings
warnings.simplefilter('ignore')

#download data
aapl = pdr.DataReader('AAPL', 'yahoo', start=datetime(2014, 8, 1), end=datetime(2016, 11, 30))

aapl.head()

#export and save as csv files
aapl.to_csv('Apple_stock.csv', sep=',')

plt.figure(figsize=(16,8))
plt.plot(aapl['Close'], label='Close Price history')
plt.legend()
plt.title('Original Data')

from statsmodels.tsa.seasonal import seasonal_decompose

result = seasonal_decompose(aapl["Close"], model='multiplicative', freq = 30)
fig = plt.figure()  
fig = result.plot()  
fig.set_size_inches(10, 8)

def adf_test(atr):
    #Perform Dickey-Fuller test:
    timeseries = aapl[atr].dropna()
    print ('Results of Dickey-Fuller Test for ',atr,'\n')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
       dfoutput['Critical Value (%s)'%key] = value
    print(dfoutput)

#apply adf test on the series
adf_test('Close')

"""ADF test: The test statistic is greater than the critical value, so we fail to reject the null hypothesis. So it is non-stationary series. Also P value is greater than 0.05 so, from that also we can say it in non-stationary."""

def kpss_test(atr):
    timeseries = aapl[atr].dropna()
    print ('Results of KPSS Test for ',atr)
    kpsstest = kpss(timeseries, regression='c')
    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])
    for key,value in kpsstest[3].items():
        kpss_output['Critical Value (%s)'%key] = value
    print (kpss_output)
kpss_test('Close')

"""KPSS Test: The test statistic is greater than the critical value so we can reject the null hypothesis. So it is non-stationary series."""

plot_acf(aapl['Close'].dropna(), lags=10)
plt.show()
plot_pacf(aapl['Close'].dropna(), lags=10)
plt.show()

"""From ACF plot also, we can see that at each lag autocorelation decreasing slowly. So data is non stationary and to make it stationary we can use differencing method. 

Here, I have used simple differencing and plotted the acf,pacf again to see status of stationarity.

"""

aapl['Diff'] = aapl['Close'].diff(periods=1)

adf_test('Diff')

kpss_test('Diff')

plot_acf(aapl['Diff'].dropna(), lags=10)
plt.show()
plot_pacf(aapl['Diff'].dropna(), lags=10)
plt.show()

"""## **ARIMA Model**"""

from statsmodels.tsa.arima_model import ARIMA

!pip3 install pmdarima

data = aapl['Close'].to_numpy()
train = data[:int(len(data)*0.8)]
test = data[int(len(data)*0.8):]
date = (aapl.index)

import pmdarima as pm
st_model = pm.auto_arima(train, start_p=1, start_q=1,max_p=3, max_q=3, m=1,start_P=0, seasonal=False,d=1, D=0, trace=True,error_action='ignore',suppress_warnings=True,stepwise=True)

model=ARIMA(data,order=(1,1,1))
model_fit = model.fit()

forecast = model_fit.predict(typ='levels')

rmse1 = np.sqrt(np.mean(np.power((np.array(test[1:])-np.array(forecast[int(len(data)*0.8):])),2)))
print('RMSE value using ARIMA model',rmse1)

plt.figure(figsize=(16,8))
plt.plot(date,data, label='Original Data')
plt.plot(date[:int(0.8*len(data))],forecast[:int(0.8*len(data))], label='Predicted Data')
plt.plot(date[int(0.8*len(data))+1:],forecast[int(0.8*len(data)):], label='Predicted Data(20%)')
plt.legend()
plt.title('Prediction using ARIMA')

"""### Fitting only train data to the ARIMA model"""

model0 = ARIMA(train,order=(1,1,1))
model_fit0 = model0.fit()

forecast0 = model_fit0.predict(len(train),len(train)+len(test)-1,typ='levels')

rmse0 = np.sqrt(np.mean(np.power((np.array(test)-np.array(forecast0)),2)))
print('RMSE value using ARIMA model (train data only): ',rmse0)

plt.figure(figsize=(16,8))
plt.plot(date,data, label='Original Data')
plt.plot(date[int(0.8*len(data)):],forecast0, label='Predicted Data')
plt.legend()
plt.title('Prediction using ARIMA')

"""## **Exponential Smoothing**"""

from statsmodels.tsa.holtwinters import ExponentialSmoothing

n = int(len(aapl["Close"])*0.8)
data = aapl['Close'].to_numpy()
train2 = data[:n]
test2 = data[n:]
date = (aapl.index)

Exp_model = ExponentialSmoothing(aapl.Close,trend='mul',seasonal='mul',seasonal_periods=4)
aapl['Pred_Exp'] = Exp_model.fit(smoothing_level = 0.9,smoothing_slope= 0.1,smoothing_seasonal = 0.2).fittedvalues.shift(0)

rmse2 = np.sqrt(np.mean(np.power((np.array(test2)-np.array(aapl.Pred_Exp[n:])),2)))
print('RMSE value using Exponential Smoothing model: ',rmse2)

plt.figure(figsize=(16,8))
plt.plot(date,data, label='Original Data')
plt.plot(date[:n],aapl.Pred_Exp[:n], label='Predicted Data')
plt.plot(date[n:],aapl.Pred_Exp[n:], label='Predicted Data(20%)')
plt.legend()
plt.title('Prediction using Exponential Smoothing')

"""## **LSTM Model**"""

#importing required libraries
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM

df1=aapl['Close']
date = (aapl.index)

scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))

##splitting dataset into train and test split
training_size = int(0.8*len(df1))
test_size = len(df1)-training_size
train_data = df1[0:training_size]
test_data = df1[training_size:len(df1)]

# convert an array of values into a dataset matrix 
def create_dataset(dataset, time_step=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-time_step-1):
		a = dataset[i:(i+time_step), 0]   
		dataX.append(a)
		dataY.append(dataset[i + time_step, 0]) 
	return np.array(dataX), np.array(dataY)

# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 5
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)

# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)

model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')

model.summary()

model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1)

# prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)

##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)

Y_train = scaler.inverse_transform(y_train.reshape(-1, 1))
Y_test = scaler.inverse_transform(ytest.reshape(-1, 1))

# Train data RMSE
rmse3 = np.sqrt(np.mean(np.power((np.array(Y_train)-np.array(train_predict)),2)))
print('RMSE value of train data using LSTM model',rmse3)

# Test Data RMSE
rmse4 = np.sqrt(np.mean(np.power((np.array(Y_test)-np.array(test_predict)),2)))
print('RMSE value of test data using LSTM model',rmse4)

# plot baseline and predictions
look_back=5
plt.figure(figsize=(16,8))
plt.plot(date,scaler.inverse_transform(df1), label='Original Data')
plt.plot(date[look_back+1:training_size],train_predict, label='Train Data Prediction')
plt.plot(date[training_size+look_back+1:],test_predict, label='Test Data Prediction')
plt.legend()
plt.title('Prediction using LSTM')
plt.show()

"""## **Inference**

For a taken parameter, RMSE value on test data using 

(1) ARIMA Model: 0.339520427471949

(2) Exponential Smoothing: 0.3859302041134889

(3) LSTM: 0.6171199441917113

 So from this and plot we can say that ARIMA predicts better for a given parametr. In actual, above sentence can not be said. Statistical models like ARIMA , Exponential smoothing are not training the model like DL,ML model. 

 Also, if we give high time-step in LSTM, It can predict much more accurate. So from  this conclusion is that deep learning models can be improved and can predict efficiently compare to stats model.
"""